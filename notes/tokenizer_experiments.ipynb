{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ce1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from cs336_basics import ROOT_DIR\n",
    "from cs336_basics.bpe.tokenization import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63271f4",
   "metadata": {},
   "source": [
    "# Sample Dataset and Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b905b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_document_boundaries(filepath:str, split_token:bytes, chunk_size:int=4096):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        file_size = f.tell()\n",
    "        pos = 0\n",
    "        document_boundaries = [0]\n",
    "        while pos < file_size:\n",
    "            f.seek(pos)\n",
    "            chunk = f.read(chunk_size)\n",
    "            found_at = chunk.find(split_token)\n",
    "            if found_at!=-1:\n",
    "                document_boundaries.append(pos+found_at+len(split_token)+1)\n",
    "                pos = document_boundaries[-1]\n",
    "            else:\n",
    "                pos += chunk_size\n",
    "    return document_boundaries\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    filepath:str, \n",
    "    num_documents:int=-1, # -1 means load all + shuffle \n",
    "    split_token:bytes=b\"<|endoftext|>\"\n",
    "):\n",
    "    doc_boundaries = find_document_boundaries(filepath, split_token)\n",
    "    print(\"Dataset size: \", len(doc_boundaries)-1)\n",
    "    if num_documents>0:\n",
    "        sampled_indices = rng.choice(len(doc_boundaries), size=num_documents, replace=False)\n",
    "    else:\n",
    "        sampled_indices = list(range(len(doc_boundaries)-1))\n",
    "    documents = []\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for index in sampled_indices:\n",
    "            f.seek(doc_boundaries[index])\n",
    "            raw_bytes = f.read(doc_boundaries[index+1]-doc_boundaries[index])\n",
    "            documents.append(raw_bytes.decode(\"utf-8\", errors=\"ignore\"))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ce8f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  2394936\n",
      "Dataset size:  2717685\n"
     ]
    }
   ],
   "source": [
    "owt_dataset = load_dataset(\n",
    "    os.path.join(ROOT_DIR, \"../data/owt_train.txt\"),\n",
    "    num_documents = 10\n",
    ")\n",
    "tiny_stories_dataset = load_dataset(\n",
    "    os.path.join(ROOT_DIR, \"../data/TinyStoriesV2-GPT4-train.txt\"),\n",
    "    num_documents = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd98d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokenizer = Tokenizer.from_files(\n",
    "    os.path.join(ROOT_DIR, \"../data/owt-train-vocab.json\"), \n",
    "    os.path.join(ROOT_DIR, \"../data/owt-train-merges.txt\"),\n",
    "    [\"<|endoftext|>\"]\n",
    ")\n",
    "tiny_stories_tokenizer = Tokenizer.from_files(\n",
    "    os.path.join(ROOT_DIR, \"../data/TinyStoriesV2-GPT4-train-vocab.json\"), \n",
    "    os.path.join(ROOT_DIR, \"../data/TinyStoriesV2-GPT4-train-merges.txt\"),\n",
    "    [\"<|endoftext|>\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93aae2",
   "metadata": {},
   "source": [
    "# 2.7 Problem `tokenizer_experiments`: Experiments with tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1bc6b",
   "metadata": {},
   "source": [
    "Compression ratio of a string is computed by:\n",
    "\n",
    "$$\\frac{\\text{number of bytes in the original string}}{\\text{number of tokens in the encoded string}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0301c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_compression_ratio(documents:list[str], tokenizer: Tokenizer):\n",
    "    encoded_length = []\n",
    "    doc_bytes_length = []\n",
    "    for doc in documents:\n",
    "        encoded_length.append(len(tokenizer.encode(doc)))\n",
    "        doc_bytes_length.append(len(doc.encode(\"utf-8\")))\n",
    "    return sum(doc_bytes_length)/sum(encoded_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337315c",
   "metadata": {},
   "source": [
    "## Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4e0f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OWTs' compression ratio: 4.366535671100363\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"OWTs' compression ratio: {compute_compression_ratio(owt_dataset, owt_tokenizer)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47d0911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tiny Stories' compression ratio: 4.104834849210149\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Tiny Stories' compression ratio: {compute_compression_ratio(tiny_stories_dataset, tiny_stories_tokenizer)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed9283",
   "metadata": {},
   "source": [
    "## Part (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8e3a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OWT's compression ratio with TinyStories tokenizer: 3.356649044326962\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"OWT's compression ratio with TinyStories tokenizer: {compute_compression_ratio(owt_dataset, tiny_stories_tokenizer)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302492e",
   "metadata": {},
   "source": [
    "When used to tokenize OWT data, the compression ratio is less when using TS tokenizer as compared to OWT tokenizer. \n",
    "\n",
    "This makes sense since frequently occurring byte-pairs in the OWT dataset would likely be merged during the bpe training procedure for the OWT tokenizer. \n",
    "This is not the case for TS tokenizer, which is trained on a different dataset with a presumably different distribution of frequently occurring byte-pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc0563",
   "metadata": {},
   "source": [
    "## Part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4916c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_throughput(documents:list[str], tokenizer:Tokenizer):\n",
    "    time_taken = []\n",
    "    doc_byte_length = []\n",
    "    for doc in documents:\n",
    "        start = time.time()\n",
    "        tokenizer.encode(doc)\n",
    "        time_taken.append(time.time()-start)\n",
    "        doc_byte_length.append(len(doc.encode(\"utf-8\")))\n",
    "    return sum(doc_byte_length)/sum(time_taken) # in bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038d86af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimated throughput of OWT tokenizer: 3280874.262319939 bytes/second'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt_throughput=estimate_throughput(owt_dataset, owt_tokenizer)\n",
    "f\"Estimated throughput of OWT tokenizer: {owt_throughput} bytes/second\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31828b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimated time to tokenize 825GB (e.g., the Pile dataset) with OWT tokenizer: 2514.573659450863 seconds'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Estimated time to tokenize 825GB (e.g., the Pile dataset) with OWT tokenizer: {8.25e9/owt_throughput} seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2fee4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimated throughput of TinyStories tokenizer: 1967621.6860878605 bytes/second'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_throughput = estimate_throughput(tiny_stories_dataset, tiny_stories_tokenizer)\n",
    "f\"Estimated throughput of TinyStories tokenizer: {ts_throughput} bytes/second\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa98efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimated time to tokenize 825GB (e.g., the Pile dataset) with TinyStories tokenizer: 4192.879179128752 seconds'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Estimated time to tokenize 825GB (e.g., the Pile dataset) with TinyStories tokenizer: {8.25e9/ts_throughput} seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd062d1",
   "metadata": {},
   "source": [
    "## Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af533589",
   "metadata": {},
   "source": [
    "Serializing token IDs as `uint16` makes sense because:\n",
    "\n",
    "1. All vocabs are assigned positive integer IDs, so unsigned integer make sense.\n",
    "\n",
    "2. TinyStories tokenizer has 10k vocab size (hence ID goes from 0 to 9999), while OWT tokenizer has 32k vocab size (hence ID goes from 0 to 31999).\n",
    "`uint16` takes up 16 bits, so it has a range of 0-65535, which is more than enough for both tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d19f50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize dataset (in string format) into token IDs, and save\n",
    "def serialize_and_save(str_dataset_path:str, tokenizer: Tokenizer, save_path:str):\n",
    "    documents = load_dataset(str_dataset_path, num_documents=-1)\n",
    "    int_documents = np.empty(len(documents), dtype=object)\n",
    "    for i, doc in enumerate(documents):\n",
    "        encoded_ids = tokenizer.encode(doc)\n",
    "        int_documents[i] = np.array(encoded_ids, dtype=np.uint16)\n",
    "    np.save(save_path, int_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb26d11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  58942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  2394936\n",
      "Dataset size:  2717685\n",
      "Dataset size:  27630\n"
     ]
    }
   ],
   "source": [
    "dataset_paths = [\"../data/owt_valid.txt\",\"../data/owt_train.txt\", \"../data/TinyStoriesV2-GPT4-train.txt\", \"../data/TinyStoriesV2-GPT4-valid.txt\" ]\n",
    "tokenizers = [owt_tokenizer, owt_tokenizer, tiny_stories_tokenizer,tiny_stories_tokenizer ]\n",
    "save_paths = [\"../data/owt_valid_encoded.npy\", \"../data/owt_train_encoded.npy\", \"../data/TinyStories_train_encoded.npy\", \"../data/TinyStories_valid_encoded.npy\"]\n",
    "for dataset_path, tokenizer, save_path in zip(dataset_paths, tokenizers, save_paths):\n",
    "    serialize_and_save(\n",
    "        os.path.join(ROOT_DIR, dataset_path),\n",
    "        tokenizer,\n",
    "        os.path.join(ROOT_DIR, save_path)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
